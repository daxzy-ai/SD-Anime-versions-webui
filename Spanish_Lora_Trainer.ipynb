{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmCPmqFL6hCQ"
      },
      "source": [
        "# â­ Entrenador de Lora de Hollowstrawberry\n",
        "\n",
        "Este colab viene de [esta guÃ­a](https://huggingface.co/hollowstrawberry/stable-diffusion-guide/blob/main/spanish.md#index). Te guiarÃ¡ para obtener tus imÃ¡genes y descripciones rÃ¡pidamente, para luego usarlas para entrenar un Lora.\n",
        "\n",
        "Basado en el trabajo de [Kohya_ss](https://github.com/kohya-ss/sd-scripts) y [Linaqruf](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb#scrollTo=-Z4w3lfFKLjr). Â¡Gracias!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y144kNjGN-jN"
      },
      "source": [
        "| |GitHub|ğŸ‡¬ğŸ‡§ English|ğŸ‡ªğŸ‡¸ Spanish|\n",
        "|:--|:-:|:-:|:-:|\n",
        "| ğŸ“Š **Dataset Maker** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Dataset_Maker.ipynb) |\n",
        "| â­ **Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Lora_Trainer.ipynb) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OglZzI_ujZq-",
        "outputId": "17a1b393-6ab1-48dc-ab69-1924884aaca5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ’¿ Revisando archivos...\n",
            "ğŸ“MyDrive/lora_training/datasets/Sofia\n",
            "ğŸ“ˆ Se encontraron 112 imÃ¡genes con 50 repeticiones, equivalente a 5600 pasos.\n",
            "ğŸ“‰ Divide 5600 pasos en 3 batch size para obtener 1866.6666666666667 pasos por epoch.\n",
            "ğŸ”® HabrÃ¡ 10 epochs, para un total de alrededor de 18666 pasos totales.\n",
            "\n",
            "ğŸ­ Instalando...\n",
            "\n",
            "Cloning into '/content/kohya-trainer'...\n",
            "remote: Enumerating objects: 2210, done.\u001b[K\n",
            "remote: Counting objects: 100% (496/496), done.\u001b[K\n",
            "remote: Compressing objects: 100% (175/175), done.\u001b[K\n",
            "remote: Total 2210 (delta 353), reused 440 (delta 321), pack-reused 1714\u001b[K\n",
            "Receiving objects: 100% (2210/2210), 2.99 MiB | 25.13 MiB/s, done.\n",
            "Resolving deltas: 100% (1490/1490), done.\n",
            "HEAD is now at f037b09 Merge pull request #360 from kohya-ss/dev\n",
            "24 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "The following NEW packages will be installed:\n",
            "  libunwind-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 472 kB of archives.\n",
            "After this operation, 4,164 kB of additional disk space will be used.\n",
            "Selecting previously unselected package libunwind-dev:amd64.\n",
            "(Reading database ... 122349 files and directories currently installed.)\n",
            "Preparing to unpack .../libunwind-dev_1.2.1-9ubuntu0.1_amd64.deb ...\n",
            "Unpacking libunwind-dev:amd64 (1.2.1-9ubuntu0.1) ...\n",
            "Setting up libunwind-dev:amd64 (1.2.1-9ubuntu0.1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "deb-libs.zip        100%[===================>]   2.23M  --.-KB/s    in 0.03s   \n",
            "Selecting previously unselected package aria2.\n",
            "(Reading database ... 122402 files and directories currently installed.)\n",
            "Preparing to unpack .../aria2_1.35.0-1build1_amd64.deb ...\n",
            "Unpacking aria2 (1.35.0-1build1) ...\n",
            "\u001b[1mdpkg:\u001b[0m \u001b[1;33mwarning:\u001b[0m downgrading google-perftools from 2.7-1ubuntu2 to 2.5-2.2ubuntu3\n",
            "Preparing to unpack .../google-perftools_2.5-2.2ubuntu3_all.deb ...\n",
            "Unpacking google-perftools (2.5-2.2ubuntu3) over (2.7-1ubuntu2) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../libaria2-0_1.35.0-1build1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.35.0-1build1) ...\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "Preparing to unpack .../libc-ares2_1.15.0-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.15.0-1ubuntu0.1) ...\n",
            "\u001b[1mdpkg:\u001b[0m \u001b[1;33mwarning:\u001b[0m downgrading libgoogle-perftools4:amd64 from 2.7-1ubuntu2 to 2.5-2.2ubuntu3\n",
            "Preparing to unpack .../libgoogle-perftools4_2.5-2.2ubuntu3_amd64.deb ...\n",
            "Unpacking libgoogle-perftools4 (2.5-2.2ubuntu3) over (2.7-1ubuntu2) ...\n",
            "Selecting previously unselected package libgoogle-perftools-dev.\n",
            "Preparing to unpack .../libgoogle-perftools-dev_2.5-2.2ubuntu3_amd64.deb ...\n",
            "Unpacking libgoogle-perftools-dev (2.5-2.2ubuntu3) ...\n",
            "Selecting previously unselected package liblz4-tool.\n",
            "Preparing to unpack .../liblz4-tool_1.9.2-2ubuntu0.20.04.1_all.deb ...\n",
            "Unpacking liblz4-tool (1.9.2-2ubuntu0.20.04.1) ...\n",
            "\u001b[1mdpkg:\u001b[0m \u001b[1;33mwarning:\u001b[0m downgrading libtcmalloc-minimal4:amd64 from 2.7-1ubuntu2 to 2.5-2.2ubuntu3\n",
            "Preparing to unpack .../libtcmalloc-minimal4_2.5-2.2ubuntu3_amd64.deb ...\n",
            "Unpacking libtcmalloc-minimal4 (2.5-2.2ubuntu3) over (2.7-1ubuntu2) ...\n",
            "Selecting previously unselected package lz4.\n",
            "Preparing to unpack .../lz4_1.9.2-2ubuntu0.20.04.1_amd64.deb ...\n",
            "Unpacking lz4 (1.9.2-2ubuntu0.20.04.1) ...\n",
            "Setting up libc-ares2:amd64 (1.15.0-1ubuntu0.1) ...\n",
            "Setting up libtcmalloc-minimal4 (2.5-2.2ubuntu3) ...\n",
            "Setting up lz4 (1.9.2-2ubuntu0.20.04.1) ...\n",
            "Setting up libaria2-0:amd64 (1.35.0-1build1) ...\n",
            "Setting up libgoogle-perftools4 (2.5-2.2ubuntu3) ...\n",
            "Setting up libgoogle-perftools-dev (2.5-2.2ubuntu3) ...\n",
            "Setting up liblz4-tool (1.9.2-2ubuntu0.20.04.1) ...\n",
            "Setting up aria2 (1.35.0-1build1) ...\n",
            "Setting up google-perftools (2.5-2.2ubuntu3) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m191.5/191.5 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m123.5/123.5 KB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m503.1/503.1 KB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m825.8/825.8 KB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m123.8/123.8 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.1/200.1 KB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m519.2/519.2 KB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for lycoris_lora (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for library (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\n",
            "âœ… InstalaciÃ³n completada en 67 segundos.\n",
            "\n",
            "ğŸ”„ Descargando modelo...\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "f291de|\u001b[1;32mOK\u001b[0m  |   223MiB/s|//content/animefull-final-pruned-fp16.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/safetensors/torch.py:98: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  with safe_open(filename, framework=\"pt\", device=device) as f:\n",
            "/usr/local/lib/python3.9/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "/usr/local/lib/python3.9/dist-packages/torch/storage.py:899: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  storage = cls(wrap_storage=untyped_storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“„ ConfiguraciÃ³n guardada en /content/drive/MyDrive/lora_training/config/Sofia/training_config.toml\n",
            "ğŸ“„ ConfiguraciÃ³n de datos guardada en /content/drive/MyDrive/lora_training/config/Sofia/dataset_config.toml\n",
            "\n",
            "â­ Iniciando entrenador...\n",
            "\n",
            "Loading settings from /content/drive/MyDrive/lora_training/config/Sofia/training_config.toml...\n",
            "/content/drive/MyDrive/lora_training/config/Sofia/training_config\n",
            "prepare tokenizer\n",
            "Downloading (â€¦)olve/main/vocab.json: 100% 961k/961k [00:00<00:00, 48.8MB/s]\n",
            "Downloading (â€¦)olve/main/merges.txt: 100% 525k/525k [00:00<00:00, 833kB/s]\n",
            "Downloading (â€¦)cial_tokens_map.json: 100% 389/389 [00:00<00:00, 91.5kB/s]\n",
            "Downloading (â€¦)okenizer_config.json: 100% 905/905 [00:00<00:00, 237kB/s]\n",
            "update token length: 225\n",
            "Load dataset config from /content/drive/MyDrive/lora_training/config/Sofia/dataset_config.toml\n",
            "prepare images.\n",
            "found directory /content/drive/MyDrive/lora_training/datasets/Sofia contains 112 image files\n",
            "5600 train images with repeating.\n",
            "0 reg images.\n",
            "no regularization images / æ­£å‰‡åŒ–ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\n",
            "[Dataset 0]\n",
            "  batch_size: 3\n",
            "  resolution: (512, 512)\n",
            "  enable_bucket: True\n",
            "  min_bucket_reso: 256\n",
            "  max_bucket_reso: 1024\n",
            "  bucket_reso_steps: 64\n",
            "  bucket_no_upscale: False\n",
            "\n",
            "  [Subset 0 of Dataset 0]\n",
            "    image_dir: \"/content/drive/MyDrive/lora_training/datasets/Sofia\"\n",
            "    image_count: 112\n",
            "    num_repeats: 50\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 1\n",
            "    caption_dropout_rate: 0.0\n",
            "    caption_dropout_every_n_epoches: 0\n",
            "    caption_tag_dropout_rate: 0.0\n",
            "    color_aug: False\n",
            "    flip_aug: False\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    token_warmup_min: 1,\n",
            "    token_warmup_step: 0,\n",
            "    is_reg: False\n",
            "    class_tokens: None\n",
            "    caption_extension: .txt\n",
            "\n",
            "\n",
            "[Dataset 0]\n",
            "loading image sizes.\n",
            "100% 112/112 [00:00<00:00, 861.58it/s]\n",
            "make buckets\n",
            "number of images (including repeats) / å„bucketã®ç”»åƒæšæ•°ï¼ˆç¹°ã‚Šè¿”ã—å›æ•°ã‚’å«ã‚€ï¼‰\n",
            "bucket 0: resolution (320, 704), count: 150\n",
            "bucket 1: resolution (320, 768), count: 50\n",
            "bucket 2: resolution (384, 640), count: 700\n",
            "bucket 3: resolution (448, 576), count: 2800\n",
            "bucket 4: resolution (512, 512), count: 550\n",
            "bucket 5: resolution (576, 448), count: 1250\n",
            "bucket 6: resolution (640, 384), count: 100\n",
            "mean ar error (without repeats): 0.05857573666393355\n",
            "prepare accelerator\n",
            "Using accelerator 0.15.0 or above.\n",
            "loading model for process 0/1\n",
            "load StableDiffusion checkpoint\n",
            "loading u-net: <All keys matched successfully>\n",
            "loading vae: <All keys matched successfully>\n",
            "Downloading (â€¦)lve/main/config.json: 100% 4.52k/4.52k [00:00<00:00, 1.31MB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.71G/1.71G [00:07<00:00, 220MB/s]\n",
            "loading text encoder: <All keys matched successfully>\n",
            "loading text encoder: <All keys matched successfully>\n",
            "Replace CrossAttention.forward to use xformers\n",
            "[Dataset 0]\n",
            "caching latents.\n",
            "  0% 0/112 [00:00<?, ?it/s]Replace CrossAttention.forward to use xformers\n",
            "[Dataset 0]\n",
            "caching latents.\n",
            "  4% 5/112 [00:10<00:23,  4.52it/s]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import toml\n",
        "import shutil\n",
        "import zipfile\n",
        "from time import time\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "if \"model_url\" in globals():\n",
        "  old_model_url = model_url\n",
        "else:\n",
        "  old_model_url = None\n",
        "if \"dependencies_installed\" not in globals():\n",
        "  dependencies_installed = False\n",
        "if \"model_file\" not in globals():\n",
        "  model_file = None\n",
        "if \"custom_dataset\" not in globals():\n",
        "  custom_dataset = None\n",
        "if \"override_dataset_config_file\" not in globals():\n",
        "  override_dataset_config_file = None\n",
        "if \"override_config_file\" not in globals():\n",
        "  override_config_file = None\n",
        "\n",
        "COLAB = True # low ram\n",
        "COMMIT = \"f037b09c2de13df549290b7c8d4d4a22ab165c36\"\n",
        "BETTER_EPOCH_NAMES = True\n",
        "LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "#@title ## ğŸš© Empieza AquÃ­\n",
        "\n",
        "#@markdown ### â–¶ï¸ Base\n",
        "#@markdown El nombre de tu proyecto tambiÃ©n es el nombre de la carpeta donde irÃ¡n tus imÃ¡genes. No se permiten espacios.\n",
        "nombre_proyecto = \"\" #@param {type:\"string\"}\n",
        "project_name = nombre_proyecto\n",
        "#@markdown La estructura de carpetas no importa y es por comodidad. AsegÃºrate de siempre elegir la misma. Me gusta organizar por proyecto.\n",
        "estructura_de_carpetas = \"Organizar por categorÃ­a (MyDrive/lora_training/datasets/nombre_proyecto)\" #@param [\"Organizar por categorÃ­a (MyDrive/lora_training/datasets/nombre_proyecto)\", \"Organizar por proyecto (MyDrive/Loras/nombre_proyecto/dataset)\"]\n",
        "folder_structure = estructura_de_carpetas\n",
        "#@markdown Decidir el modelo base de entrenamiento. Los modelos por defecto producen los resultados mÃ¡s limpios y consistentes. Puedes cambiarlo por un modelo propio si lo deseas.\n",
        "modelo_de_entrenamiento = \"Anime (animefull-final-pruned-fp16.safetensors)\" #@param [\"Anime (animefull-final-pruned-fp16.safetensors)\", \"Fotorealismo (sd-v1-5-pruned-noema-fp16.safetensors)\"]\n",
        "opcional_enlace_a_modelo_propio = \"\" #@param {type:\"string\"}\n",
        "modelo_propio_basado_en_sd2 = False #@param {type:\"boolean\"}\n",
        "custom_model_is_based_on_sd2 = modelo_propio_basado_en_sd2\n",
        "#markdown UbicaciÃ³n al archivo de un Lora existente para continuar el entrenamiento. **Advertencia:** No es lo mismo que entrenar de corrido. Los epochs empiezan desde cero y puede tener peores resultados.\n",
        "continuar_lora = \"\" #param {type:\"string\"}\n",
        "continue_from_lora = continuar_lora\n",
        "\n",
        "if continue_from_lora and not continue_from_lora.startswith(\"/content/drive/MyDrive\"):\n",
        "  continue_from_lora = os.path.join(\"/content/drive/MyDrive\", continue_from_lora)\n",
        "\n",
        "if opcional_enlace_a_modelo_propio:\n",
        "  model_url = opcional_enlace_a_modelo_propio\n",
        "elif \"Anime\" in modelo_de_entrenamiento:\n",
        "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/animefull-final-pruned-fp16.safetensors\"\n",
        "else:\n",
        "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/sd-v1-5-pruned-noema-fp16.safetensors\"\n",
        "\n",
        "#@markdown ### â–¶ï¸ Procesamiento <p>\n",
        "#@markdown La resoluciÃ³n de 512 es estÃ¡ndar en Stable Diffusion 1.5. No es necesario recortar o achicar, el proceso es automÃ¡tico.\n",
        "resolucion = 512 #@param {type:\"number\"}\n",
        "resolution = resolucion\n",
        "#@markdown Esta opciÃ³n va a voltear tus imÃ¡genes para asÃ­ tener el doble y aprender mejor. <p>\n",
        "#@markdown **Desactiva esto si te importan los elementos asimÃ©tricos en tu Lora.**\n",
        "flip_aug = False #@param {type:\"boolean\"}\n",
        "#markdown Leave empty for no captions.\n",
        "caption_extension = \".txt\" #param {type:\"string\"}\n",
        "#@markdown Mezclar las tags de anime ayuda al aprendizaje. Una tag de activaciÃ³n va al inicio de cada archivo de texto y no se mezclarÃ¡.\n",
        "mezclar_tags = True #@param {type:\"boolean\"}\n",
        "shuffle_caption = mezclar_tags\n",
        "tags_de_activacion = \"1\" #@param [0,1,2,3]\n",
        "keep_tokens = int(tags_de_activacion)\n",
        "\n",
        "#@markdown ### â–¶ï¸ Pasos <p>\n",
        "#@markdown Tus imÃ¡genes se repetirÃ¡n este nÃºmero de veces durante el entrenamiento. Recomiendo que el valor total sea entre 200 y 400.\n",
        "num_repeats = 10 #@param {type:\"number\"}\n",
        "#@markdown CuÃ¡nto tiempo deseas entrenar. Un buen punto de partida puede ser alrededor de 10 epochs o alrededor de 2000 pasos. <p>\n",
        "#@markdown Un epoch es una cantidad de pasos igual a: tu cantidad de imÃ¡genes multipliccada por sus repeticiones, y dividido en el batch size.\n",
        "unidad_preferida = \"Epochs\" #@param [\"Epochs\", \"Pasos\"]\n",
        "cuantos = 10 #@param {type:\"number\"}\n",
        "max_train_epochs = cuantos if unidad_preferida == \"Epochs\" else None\n",
        "max_train_steps = cuantos if unidad_preferida == \"Pasos\" else None\n",
        "#@markdown Guardar mÃ¡s epochs te permitirÃ¡ comparar mejor el progreso de tu Lora.\n",
        "guardar_cada_cuantos_epochs = 1 #@param {type:\"number\"}\n",
        "save_every_n_epochs = guardar_cada_cuantos_epochs\n",
        "guardar_solo_ultimos_epochs = 5 #@param {type:\"number\"}\n",
        "keep_last_n_epochs = guardar_solo_ultimos_epochs\n",
        "if not save_every_n_epochs:\n",
        "  save_every_n_epochs = max_train_epochs\n",
        "if not keep_last_n_epochs:\n",
        "  keep_last_n_epochs = max_train_epochs\n",
        "#@markdown Un batch size mayor hace el entrenamiento mÃ¡s rÃ¡pido, pero puede empeorar el aprendizaje. Se recomienda 2 o 3.\n",
        "batch_size = 2 #@param {type:\"slider\", min:1, max:8, step:1}\n",
        "train_batch_size = batch_size\n",
        "\n",
        "#@markdown ### â–¶ï¸ Aprendizaje\n",
        "#@markdown La tasa de aprendizaje es lo mÃ¡s importante. Si deseas entrenar mÃ¡s lento con muchas imÃ¡genes, o si tienes un dim y alpha altos, usa un unet de 2e-4 o menor. <p>\n",
        "#@markdown El text encoder ayuda a tu Lora a aprender conceptos un poco mejor. Se recomienda la mitad o un quinto del unet. Puedes dejarlo en 0 para algunos estilos.\n",
        "aprendizaje_unet = 5e-4 #@param {type:\"number\"}\n",
        "unet_lr = aprendizaje_unet\n",
        "aprendizaje_text_encoder = 1e-4 #@param {type:\"number\"}\n",
        "text_encoder_lr = aprendizaje_text_encoder\n",
        "#@markdown El scheduler es el algoritmo matemÃ¡tico que guiarÃ¡ el entrenamiento. Para personajes recomiendo `cosine_with_restarts` con un valor de 3. Si no estÃ¡s seguro ponlo en `constant` e ignora el valor.\n",
        "scheduler = \"cosine_with_restarts\" #@param [\"constant\", \"cosine\", \"cosine_with_restarts\", \"constant_with_warmup\", \"linear\", \"polynomial\"]\n",
        "lr_scheduler = scheduler\n",
        "valor_de_scheduler = 3 #@param {type:\"number\"}\n",
        "lr_scheduler_number = valor_de_scheduler\n",
        "lr_scheduler_num_cycles = lr_scheduler_number if lr_scheduler == \"cosine_with_restarts\" else 0\n",
        "lr_scheduler_power = lr_scheduler_number if lr_scheduler == \"polynomial\" else 0\n",
        "#@markdown Pasos de calentamiento durante el entrenamiento para un inicio eficiente. Recomiendo dejarlo en 5%.\n",
        "calentamiento = 0.05 #@param {type:\"slider\", min:0.0, max:0.5, step:0.01}\n",
        "lr_warmup_ratio = calentamiento\n",
        "lr_warmup_steps = 0\n",
        "#@markdown Nueva funciÃ³n que hace el aprendizaje mucho mÃ¡s eficiente. Puede que tus Loras estÃ©n listos en la mitad de epochs. Se usarÃ¡ un valor de 5.0 como en la [investigaciÃ³n](https://arxiv.org/abs/2303.09556).\n",
        "min_snr_gamma = True #@param {type:\"boolean\"}\n",
        "min_snr_gamma_value = 5.0 if min_snr_gamma else None\n",
        "\n",
        "#@markdown ### â–¶ï¸ Estructura\n",
        "#@markdown LoRA es el tipo clÃ¡sico, mientras que LoCon es bueno con estilos. Lycoris requiere [esta extensiÃ³n](https://github.com/KohakuBlueleaf/a1111-sd-webui-lycoris) para funcionar como un lora normal. MÃ¡s informaciÃ³n [aquÃ­](https://github.com/KohakuBlueleaf/Lycoris).\n",
        "lora_type = \"LoRA\" #@param [\"LoRA\", \"LoCon Lycoris\", \"LoHa Lycoris\"]\n",
        "\n",
        "#@markdown AquÃ­ hay algunos valores recomendados para las configuraciones de abajo:\n",
        "\n",
        "#@markdown | type | network_dim | network_alpha | conv_dim | conv_alpha |\n",
        "#@markdown | :---: | :---: | :---: | :---: | :---: |\n",
        "#@markdown | LoRA | 32 | 16 |   |   |\n",
        "#@markdown | LoCon | 16 | 8 | 8 | 1 |\n",
        "#@markdown | LoHa | 8 | 4 | 4 | 1 |\n",
        "\n",
        "#@markdown Un dim mayor aumenta el tamaÃ±o del Lora, puede aprender mÃ¡s pero no siempre es mejor. Se recomienda un dim entre 8 y 32, y un alpha igual a la mitad del dim.\n",
        "network_dim = 16 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "network_alpha = 8 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "#@markdown Los siguientes valores no afectan a LoRA. Funcionan como el dim/alpha pero para las capas adicionales de los Lycoris.\n",
        "conv_dim = 8 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "conv_alpha = 1 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "conv_compression = False #@param {type:\"boolean\"}\n",
        "\n",
        "network_module = \"lycoris.kohya\" if \"Lycoris\" in lora_type else \"networks.lora\"\n",
        "network_args = None if lora_type == \"LoRA\" else [\n",
        "  f\"conv_dim={conv_dim}\",\n",
        "  f\"conv_alpha={conv_alpha}\",\n",
        "]\n",
        "if \"Lycoris\" in lora_type:\n",
        "  network_args.append(f\"algo={'loha' if 'LoHa' in network_args else 'lora'}\")\n",
        "  network_args.append(f\"disable_conv_cp={str(not conv_compression)}\")\n",
        "\n",
        "#markdown ### â–¶ï¸ Experimental\n",
        "#markdown Save additional data equaling ~1 GB allowing you to resume training later.\n",
        "save_state = False #param {type:\"boolean\"}\n",
        "#markdown Resume training if a save state is found.\n",
        "resume = False #param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### â–¶ï¸ Listo\n",
        "#@markdown Ahora puedes correr esta celda apretando el botÃ³n circular a la izquierda. Â¡Buena suerte!\n",
        "\n",
        "\n",
        "# ğŸ‘©â€ğŸ’» Cool code goes here\n",
        "\n",
        "root_dir = \"/content\" if COLAB else \"~/Loras\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "\n",
        "if \"/Loras\" in folder_structure:\n",
        "  main_dir      = os.path.join(root_dir, \"drive/MyDrive/Loras\") if COLAB else root_dir\n",
        "  log_folder    = os.path.join(main_dir, \"_logs\")\n",
        "  config_folder = os.path.join(main_dir, project_name)\n",
        "  images_folder = os.path.join(main_dir, project_name, \"dataset\")\n",
        "  output_folder = os.path.join(main_dir, project_name, \"output\")\n",
        "else:\n",
        "  main_dir      = os.path.join(root_dir, \"drive/MyDrive/lora_training\") if COLAB else root_dir\n",
        "  images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
        "  output_folder = os.path.join(main_dir, \"output\", project_name)\n",
        "  config_folder = os.path.join(main_dir, \"config\", project_name)\n",
        "  log_folder    = os.path.join(main_dir, \"log\")\n",
        "\n",
        "config_file = os.path.join(config_folder, \"training_config.toml\")\n",
        "dataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\n",
        "accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "\n",
        "def clone_repo():\n",
        "  os.chdir(root_dir)\n",
        "  !git clone https://github.com/kohya-ss/sd-scripts {repo_dir}\n",
        "  os.chdir(repo_dir)\n",
        "  if COMMIT:\n",
        "    !git reset --hard {COMMIT}\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/requirements.txt -q -O requirements.txt\n",
        "\n",
        "def install_ubuntu_deps(url):\n",
        "    if not COLAB:\n",
        "      !apt -y install aria2\n",
        "      return not get_ipython().__dict__['user_ns']['_exit_code']\n",
        "    !apt -y install libunwind8-dev -qq\n",
        "    os.chdir(repo_dir)\n",
        "    !wget -q --show-progress {url}\n",
        "    with zipfile.ZipFile(url[url.rfind(\"/\")+1:], \"r\") as deps:\n",
        "      deps.extractall(deps_dir)\n",
        "    !dpkg -i {deps_dir}/*\n",
        "    os.remove(url[url.rfind(\"/\")+1:])\n",
        "    shutil.rmtree(deps_dir)\n",
        "\n",
        "def install_dependencies():\n",
        "  clone_repo()\n",
        "  !apt -y update -qq\n",
        "  install_ubuntu_deps(\"https://huggingface.co/Linaqruf/fast-repo/resolve/main/deb-libs.zip\")\n",
        "  !pip -q install --upgrade -r requirements.txt\n",
        "\n",
        "  # patch kohya for minor stuff\n",
        "  if COLAB:\n",
        "    !sed -i \"s@cpu@cuda@\" library/model_util.py # low ram\n",
        "  if LOAD_TRUNCATED_IMAGES:\n",
        "    !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py # fix truncated jpegs error\n",
        "  if BETTER_EPOCH_NAMES:\n",
        "    !sed -i 's/{:06d}/{:02d}/g' library/train_util.py # make epoch names shorter\n",
        "    !sed -i 's/model_name + \".\"/model_name + \"-{:02d}.\".format(num_train_epochs)/g' train_network.py # name of the last epoch will match the rest\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  if not os.path.exists(accelerate_config_file):\n",
        "    write_basic_config(save_location=accelerate_config_file)\n",
        "\n",
        "  os.environ[\"LD_PRELOAD\"] = \"libtcmalloc.so\"\n",
        "  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "  os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"  \n",
        "  #os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda/lib64/:$LD_LIBRARY_PATH\"\n",
        "  os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "\n",
        "def validate_dataset():\n",
        "  global lr_warmup_steps, lr_warmup_ratio, caption_extension\n",
        "  supported_types = (\".png\", \".jpg\", \".jpeg\")\n",
        "\n",
        "  print(\"\\nğŸ’¿ Revisando archivos...\")\n",
        "  if not project_name.strip() or any(c in project_name for c in \" .()\\\"'\\\\/\"):\n",
        "    print(\"ğŸ’¥ Error: Por favor elije un nombre de proyecto vÃ¡lido.\")\n",
        "    return\n",
        "\n",
        "  if custom_dataset:\n",
        "    try:\n",
        "      datconf = toml.loads(custom_dataset)\n",
        "      datasets = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datconf[\"datasets\"][0][\"subsets\"]}\n",
        "    except:\n",
        "      print(f\"ğŸ’¥ Error: Â¡Tu configuraciÃ³n de datos propia es invÃ¡lida o tiene errores! Por favor revisa el ejemplo original.\")\n",
        "      return\n",
        "    folders = datasets.keys()\n",
        "    files = [f for folder in folders for f in os.listdir(folder)]\n",
        "    images_repeats = {folder: (len([f for f in os.listdir(folder) if f.lower().endswith(supported_types)]), datasets[folder]) for folder in folders}\n",
        "  else:\n",
        "    folders = [images_folder]\n",
        "    files = os.listdir(images_folder)\n",
        "    images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n",
        "\n",
        "  for folder in folders:\n",
        "    if not os.path.exists(folder):\n",
        "      print(f\"ğŸ’¥ Error: La carpeta {folder.replace('/content/drive/', '')} no existe.\")\n",
        "      return\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    if not img:\n",
        "      print(f\"ğŸ’¥ Error: La carpeta {folder.replace('/content/drive/', '')} estÃ¡ vacÃ­a.\")\n",
        "      return\n",
        "  for f in files:\n",
        "    if not f.lower().endswith(\".txt\") and not f.lower().endswith(supported_types):\n",
        "      print(f\"ğŸ’¥ Error: Archivo invÃ¡lido encontrado: \\\"{f}\\\". Abortando.\")\n",
        "      return\n",
        "    \n",
        "  if not [txt for txt in files if txt.lower().endswith(\".txt\")]:\n",
        "    caption_extension = \"\"\n",
        "  if continue_from_lora and not (continue_from_lora.endswith(\".safetensors\") and os.path.exists(continue_from_lora)):\n",
        "    print(f\"ğŸ’¥ Error: Archivo de continuar_lora invÃ¡lido. Ejemplo: /content/drive/MyDrive/lora_training/ejemplo.safetensors\")\n",
        "    return\n",
        "\n",
        "  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n",
        "  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n",
        "  total_steps = int(max_train_epochs*steps_per_epoch)\n",
        "  lr_warmup_steps = int(total_steps * lr_warmup_ratio)\n",
        "\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    print(\"ğŸ“\"+folder.replace(\"/content/drive/\", \"\"))\n",
        "    print(f\"ğŸ“ˆ Se encontraron {img} imÃ¡genes con {rep} repeticiones, equivalente a {img*rep} pasos.\")\n",
        "  print(f\"ğŸ“‰ Divide {pre_steps_per_epoch} pasos en {train_batch_size} batch size para obtener {steps_per_epoch} pasos por epoch.\")\n",
        "  print(f\"ğŸ”® HabrÃ¡ {max_train_epochs} epochs, para un total de alrededor de {total_steps} pasos totales.\")\n",
        "\n",
        "  if total_steps > 10000:\n",
        "    print(\"ğŸ’¥ Error: Tus pasos totales on muy altos. Probablemente cometiste un error. Abortando...\") \n",
        "    return\n",
        "  return True\n",
        "\n",
        "def create_config():\n",
        "  global dataset_config_file, config_file, model_file\n",
        "\n",
        "  if resume:\n",
        "    resume_points = [f.path for f in os.scandir(output_folder) if f.is_dir()]\n",
        "    resume_points.sort()\n",
        "    last_resume_point = resume_points[-1] if resume_points else None\n",
        "  else:\n",
        "    last_resume_point = None\n",
        "\n",
        "  if override_config_file:\n",
        "    config_file = override_config_file\n",
        "    print(f\"\\nâ­• Usando configuraciÃ³n propia {config_file}\")\n",
        "  else:\n",
        "    config_dict = {\n",
        "      \"additional_network_arguments\": {\n",
        "        \"unet_lr\": unet_lr,\n",
        "        \"text_encoder_lr\": text_encoder_lr,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_args\": network_args,\n",
        "        \"network_train_unet_only\": True if text_encoder_lr == 0 else None,\n",
        "        \"network_weights\": continue_from_lora if continue_from_lora else None\n",
        "      },\n",
        "      \"optimizer_arguments\": {\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps,\n",
        "        \"optimizer_type\": \"AdamW8bit\",\n",
        "      },\n",
        "      \"training_arguments\": {\n",
        "        \"max_train_epochs\": max_train_epochs,\n",
        "        \"save_every_n_epochs\": save_every_n_epochs,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"noise_offset\": None,\n",
        "        \"clip_skip\": 2,\n",
        "        \"min_snr_gamma\": min_snr_gamma_value,\n",
        "        \"seed\": 42,\n",
        "        \"max_token_length\": 225,\n",
        "        \"xformers\": True,\n",
        "        \"lowram\": COLAB,\n",
        "        \"max_data_loader_n_workers\": 8,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"save_precision\": \"fp16\",\n",
        "        \"mixed_precision\": \"fp16\",\n",
        "        \"output_dir\": output_folder,\n",
        "        \"logging_dir\": log_folder,\n",
        "        \"output_name\": project_name,\n",
        "        \"log_prefix\": project_name,\n",
        "        \"save_state\": save_state,\n",
        "        \"save_last_n_epochs_state\": 1 if save_state else None,\n",
        "        \"resume\": last_resume_point\n",
        "      },\n",
        "      \"model_arguments\": {\n",
        "        \"pretrained_model_name_or_path\": model_file,\n",
        "        \"v2\": custom_model_is_based_on_sd2,\n",
        "        \"v_parameterization\": True if custom_model_is_based_on_sd2 else None,\n",
        "      },\n",
        "      \"saving_arguments\": {\n",
        "        \"save_model_as\": \"safetensors\",\n",
        "      },\n",
        "      \"dreambooth_arguments\": {\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "      },\n",
        "      \"dataset_arguments\": {\n",
        "        \"cache_latents\": True,\n",
        "      },\n",
        "    }\n",
        "\n",
        "    for key in config_dict:\n",
        "      if isinstance(config_dict[key], dict):\n",
        "        config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(config_dict))\n",
        "    print(f\"\\nğŸ“„ ConfiguraciÃ³n guardada en {config_file}\")\n",
        "\n",
        "  if override_dataset_config_file:\n",
        "    dataset_config_file = override_dataset_config_file\n",
        "    print(f\"â­• Usando configuraciÃ³n de datos propia {dataset_config_file}\")\n",
        "  else:\n",
        "    dataset_config_dict = {\n",
        "      \"general\": {\n",
        "        \"resolution\": resolution,\n",
        "        \"shuffle_caption\": shuffle_caption,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"flip_aug\": flip_aug,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"enable_bucket\": True,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"bucket_no_upscale\": False,\n",
        "        \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
        "        \"max_bucket_reso\": 1280 if resolution > 640 else 1024,\n",
        "      },\n",
        "      \"datasets\": toml.loads(custom_dataset)[\"datasets\"] if custom_dataset else [\n",
        "        {\n",
        "          \"subsets\": [\n",
        "            {\n",
        "              \"num_repeats\": num_repeats,\n",
        "              \"image_dir\": images_folder,\n",
        "              \"class_tokens\": None if caption_extension else project_name\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "\n",
        "    for key in dataset_config_dict:\n",
        "      if isinstance(dataset_config_dict[key], dict):\n",
        "        dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(dataset_config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(dataset_config_dict))\n",
        "    print(f\"ğŸ“„ ConfiguraciÃ³n de datos guardada en {dataset_config_file}\")\n",
        "\n",
        "def download_model():\n",
        "  global old_model_url, model_url, model_file\n",
        "  real_model_url = model_url.strip()\n",
        "  \n",
        "  if real_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n",
        "    model_file = f\"/content{real_model_url[real_model_url.rfind('/'):]}\"\n",
        "  else:\n",
        "    model_file = \"/content/downloaded_model.safetensors\"\n",
        "    if os.path.exists(model_file):\n",
        "      !rm \"{model_file}\"\n",
        "\n",
        "  if m := re.search(r\"(?:https?://)?(?:www\\.)?huggingface\\.co/[^/]+/[^/]+/blob\", model_url):\n",
        "    real_model_url = real_model_url.replace(\"blob\", \"resolve\")\n",
        "  elif m := re.search(r\"(?:https?://)?(?:www\\.)?civitai\\.com/models/([0-9]+)\", model_url):\n",
        "    real_model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "\n",
        "  !aria2c \"{real_model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n",
        "\n",
        "  if model_file.lower().endswith(\".safetensors\"):\n",
        "    from safetensors.torch import load_file as load_safetensors\n",
        "    try:\n",
        "      test = load_safetensors(model_file)\n",
        "      del test\n",
        "    except Exception as e:\n",
        "      #if \"HeaderTooLarge\" in str(e):\n",
        "      new_model_file = os.path.splitext(model_file)[0]+\".ckpt\"\n",
        "      !mv \"{model_file}\" \"{new_model_file}\"\n",
        "      model_file = new_model_file\n",
        "      print(f\"El modelo ahora es {os.path.splitext(model_file)[0]}.ckpt\")\n",
        "\n",
        "  if model_file.lower().endswith(\".ckpt\"):\n",
        "    from torch import load as load_ckpt\n",
        "    try:\n",
        "      test = load_ckpt(model_file)\n",
        "      del test\n",
        "    except Exception as e:\n",
        "      return False\n",
        "      \n",
        "  return True\n",
        "\n",
        "def main():\n",
        "  global dependencies_installed\n",
        "\n",
        "  if COLAB and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    print(\"ğŸ“‚ Conectando a Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "  \n",
        "  for dir in (main_dir, deps_dir, repo_dir, log_folder, images_folder, output_folder, config_folder):\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  if not validate_dataset():\n",
        "    return\n",
        "  \n",
        "  if not dependencies_installed:\n",
        "    print(\"\\nğŸ­ Instalando...\\n\")\n",
        "    t0 = time()\n",
        "    install_dependencies()\n",
        "    t1 = time()\n",
        "    dependencies_installed = True\n",
        "    print(f\"\\nâœ… InstalaciÃ³n completada en {int(t1-t0)} segundos.\")\n",
        "  else:\n",
        "    print(\"\\nâœ… Ya se ha realizado la instalaciÃ³n.\")\n",
        "\n",
        "  if old_model_url != model_url or not model_file or not os.path.exists(model_file):\n",
        "    print(\"\\nğŸ”„ Descargando modelo...\")\n",
        "    if not download_model():\n",
        "      print(\"\\nğŸ’¥ Error: El modelo que elegiste es invÃ¡lido o estÃ¡ corrupto, o no se pudo encontrar. Recomiendo usar un enlace de huggingface o civitai.\")\n",
        "      return\n",
        "    print()\n",
        "  else:\n",
        "    print(\"\\nğŸ”„ El modelo ya ha sido descargado.\\n\")\n",
        "\n",
        "  create_config()\n",
        "  \n",
        "  print(\"\\nâ­ Iniciando entrenador...\\n\")\n",
        "  os.chdir(repo_dir)\n",
        "  \n",
        "  !accelerate launch --config_file={accelerate_config_file} --num_cpu_threads_per_process=1 train_network.py --dataset_config={dataset_config_file} --config_file={config_file}\n",
        "\n",
        "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "    display(Markdown(\"### âœ… Â¡Listo! [Descarga tus Loras desde Google Drive](https://drive.google.com/drive/my-drive)\"))\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNM2AAWaNheJ"
      },
      "source": [
        "## *ï¸âƒ£ Extras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgm5gTadzHU3"
      },
      "source": [
        "### ğŸ“š MÃºltiples carpetas\n",
        "**Para usuarios avanzados:** Antes de iniciar el entrenamiento, puedes editar y correr la celda aquÃ­ abajo, la cual tiene un ejemplo para definir tus propias carpetas de imÃ¡genes con diferentes repeticiones.\n",
        "\n",
        "(El nÃºmero de repeticiones de la celda principal serÃ¡ ignorado, y tambiÃ©n la carpeta principal con el nombre del proyecto)\n",
        "\n",
        "Puedes hacer que una carpeta contenga imÃ¡genes de regularizaciÃ³n con la frase `is_reg = true`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l54_JSVTzHU5"
      },
      "outputs": [],
      "source": [
        "custom_dataset = \"\"\"\n",
        "[[datasets]]\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/lora_training/datasets/my_lora/good_images\"\n",
        "num_repeats = 3\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/lora_training/datasets/my_lora/normal_images\"\n",
        "num_repeats = 1\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHjyd9HRzHU-"
      },
      "outputs": [],
      "source": [
        "custom_dataset = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jhfwm-iPNiNh"
      },
      "outputs": [],
      "source": [
        "#@markdown ### ğŸ“‚ Extraer datos\n",
        "#@markdown Es lento subir muchos archivos pequeÃ±os, si quieres puedes subir un zip y extraerlo aquÃ­.\n",
        "zip = \"/content/drive/MyDrive/mi_dataset.zip\" #@param {type:\"string\"}\n",
        "extract_to = \"/content/drive/MyDrive/lora_training/datasets/mi_lora\" #@param {type:\"string\"}\n",
        "\n",
        "import os, zipfile\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  print(\"ğŸ“‚ Connecting to Google Drive...\")\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip, 'r') as f:\n",
        "  f.extractall(extract_to)\n",
        "\n",
        "print(\"âœ… Done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rXRAOL7R8ofB"
      },
      "outputs": [],
      "source": [
        "#@markdown ### ğŸ”¢ Contar archivos\n",
        "#@markdown Google Drive hace imposible contar los archivos en una carpeta, por lo que aquÃ­ puedes ver la cantidad de archivos en carpetas y subcarpetas.\n",
        "carpeta = \"/content/drive/MyDrive/lora_training/datasets\" #@param {type:\"string\"}\n",
        "folder = carpeta\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"ğŸ“‚ Connecting to Google Drive...\\n\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "tree = {}\n",
        "exclude = (\"_logs\", \"/output\")\n",
        "for i, (root, dirs, files) in enumerate(os.walk(folder, topdown=True)):\n",
        "  dirs[:] = [d for d in dirs if all(ex not in d for ex in exclude)]\n",
        "  images = len([f for f in files if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
        "  captions = len([f for f in files if f.lower().endswith(\".txt\")])\n",
        "  others = len(files) - images - captions\n",
        "  path = root[folder.rfind(\"/\")+1:]\n",
        "  tree[path] = None if not images else f\"{images:>4} images | {captions:>4} captions |\"\n",
        "  if tree[path] and others:\n",
        "    tree[path] += f\" {others:>4} other files\"\n",
        "\n",
        "pad = max(len(k) for k in tree)\n",
        "print(\"\\n\".join(f\"ğŸ“{k.ljust(pad)} | {v}\" for k, v in tree.items() if v))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}